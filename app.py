# app.py
# =========================================================
# Y-Compass (2026 ìˆ˜ì‹œ) â€” PDF ê¸°ë°˜ ì „í˜• KB êµ¬ì¶•(ì •ê·œì‹+LLM í•˜ì´ë¸Œë¦¬ë“œ)
#  - PDFì—ì„œ ì „í˜•ëª…/ì „í˜•ìš”ì†Œ/ë°˜ì˜ë¹„ìœ¨/ìˆ˜ëŠ¥ìµœì €/ì¼ì • ìë™ ì¶”ì¶œ â†’ í‘œë¡œ ë³´ì—¬ì¤Œ
#  - ì¶”ì¶œ ê²°ê³¼ë¥¼ JSON DBë¡œ ì €ì¥ â†’ ì•± ì‹¤í–‰ ì‹œ ì¦‰ì‹œ ë¡œë”©
#  - ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì „í˜• ì¡°ê±´ ë§¤ì¹­ â†’ Top3 ì¶”ì²œ + 8ì£¼ ë¡œë“œë§µ
#
# requirements.txt ê¶Œì¥:
#   streamlit
#   pandas
#   pypdf
#   openai>=1.0.0
#   reportlab   # (ìˆì–´ë„ ë˜ê³  ì—†ì–´ë„ ë¨. ì´ ì•±ì€ ë¹„ì˜ì¡´)
#   # ì„ íƒ:
#   pdfplumber  # pypdfê°€ í…ìŠ¤íŠ¸ ì˜ ëª»ë½‘ëŠ” PDF ëŒ€ë¹„(í…ìŠ¤íŠ¸ PDF ê¶Œì¥)
# =========================================================

import os
import re
import json
import time
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

import streamlit as st
import pandas as pd

# -----------------------------
# App config
# -----------------------------
st.set_page_config(page_title="Y-Compass (2026 ìˆ˜ì‹œ) â€” ì „í˜• KB + ì¶”ì²œ + 8ì£¼ í”Œëœ", layout="wide")
st.title("ğŸ“ Y-Compass (2026 ìˆ˜ì‹œ) â€” ì „í˜• KB(PDFâ†’í‘œ) + ì „í˜• ì¶”ì²œ + 8ì£¼ ë¡œë“œë§µ")
st.caption("ì„œìš¸ëŒ€/ì—°ì„¸ëŒ€/ê³ ë ¤ëŒ€ ëª¨ì§‘ìš”ê°• PDFë¥¼ ì—…ë¡œë“œ â†’ ì „í˜• ì •ë³´ë¥¼ êµ¬ì¡°í™”(KB) â†’ â€˜ëª°ë¼ì„œ ëª» ì§€ì›â€™ ì¤„ì´ê³  ì „ëµ/í”Œëœê¹Œì§€ ìë™í™”.")

# -----------------------------
# Paths (Streamlit Cloudì—ì„œë„ íŒŒì¼ ì €ì¥ì€ 'ì„¸ì…˜/ì»¨í…Œì´ë„ˆ' ë‚´ì— ê°€ëŠ¥)
#   - ë°°í¬ í™˜ê²½ì— ë”°ë¼ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”ë  ìˆ˜ ìˆìŒ(=DB ë°±ì—… ë‹¤ìš´ë¡œë“œ ì œê³µ)
# -----------------------------
DATA_DIR = "data"
KB_PATH = os.path.join(DATA_DIR, "admission_kb.json")

def ensure_data_dir():
    os.makedirs(DATA_DIR, exist_ok=True)

# -----------------------------
# Optional: OpenAI
# -----------------------------
def get_openai_client(api_key: str):
    try:
        from openai import OpenAI
        return OpenAI(api_key=api_key)
    except Exception:
        return None

def llm_json(client, model: str, system: str, user: str) -> Optional[Any]:
    if client is None:
        return None
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "system", "content": system},
                      {"role": "user", "content": user}],
            temperature=0.2,
        )
        txt = resp.choices[0].message.content.strip()
        # ëª¨ë¸ì´ ì½”ë“œíœìŠ¤ ë¶™ì´ë©´ ì œê±°
        txt = re.sub(r"^```(?:json)?\s*", "", txt)
        txt = re.sub(r"\s*```$", "", txt)
        return json.loads(txt)
    except Exception:
        return None

# -----------------------------
# PDF extraction
# -----------------------------
def extract_text_from_pdf(uploaded_file) -> str:
    # pypdf ìš°ì„ 
    try:
        from pypdf import PdfReader
        uploaded_file.seek(0)
        reader = PdfReader(uploaded_file)
        texts = []
        for i, page in enumerate(reader.pages):
            t = page.extract_text() or ""
            if t.strip():
                texts.append(f"\n\n[PAGE {i+1}]\n{t}")
        joined = "\n".join(texts).strip()
        if len(joined) > 400:
            return joined
    except Exception:
        pass

    # pdfplumber fallback(ì„¤ì¹˜ëœ ê²½ìš°)
    try:
        import pdfplumber
        uploaded_file.seek(0)
        texts = []
        with pdfplumber.open(uploaded_file) as pdf:
            for i, page in enumerate(pdf.pages):
                t = page.extract_text() or ""
                if t.strip():
                    texts.append(f"\n\n[PAGE {i+1}]\n{t}")
        return "\n".join(texts).strip()
    except Exception:
        return ""

def normalize_space(t: str) -> str:
    t = t.replace("\u00a0", " ")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

# -----------------------------
# KB I/O
# -----------------------------
def load_kb() -> Dict[str, Any]:
    ensure_data_dir()
    if not os.path.exists(KB_PATH):
        return {"version": 1, "updated_at": None, "universities": {}}
    try:
        with open(KB_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {"version": 1, "updated_at": None, "universities": {}}

def save_kb(kb: Dict[str, Any]) -> None:
    ensure_data_dir()
    kb["updated_at"] = int(time.time())
    with open(KB_PATH, "w", encoding="utf-8") as f:
        json.dump(kb, f, ensure_ascii=False, indent=2)

# -----------------------------
# Heuristic parsing (Regex-first)
#   ëª©í‘œ: ì „í˜•ëª…/ìš”ì†Œ/ë°˜ì˜ë¹„ìœ¨/ìˆ˜ëŠ¥ìµœì €/ì¼ì •
# -----------------------------
TRACK_NAME_PATTERNS = [
    r"í•™ìƒë¶€\s*ì¢…í•©(?:\s*\[[^\]]+\])?",
    r"í•™ìƒë¶€\s*êµê³¼(?:\s*\[[^\]]+\])?",
    r"ë…¼ìˆ \s*ì „í˜•",
    r"ì‹¤ê¸°(?:/ì‹¤ì )?\s*ì „í˜•",
    r"íŠ¹ê¸°ì\s*ì „í˜•",
    r"ê¸°íšŒ\s*ê· í˜•(?:\s*ì „í˜•)?",
    r"ê³ ë¥¸\s*ê¸°íšŒ(?:\s*ì „í˜•)?",
    r"ì§€ì—­\s*ê· í˜•(?:\s*ì „í˜•)?",
    r"ì¼ë°˜\s*ì „í˜•",
    r"í•™êµ\s*ì¶”ì²œ(?:\s*ì „í˜•)?",
    r"í•™ì—…\s*ìš°ìˆ˜(?:\s*ì „í˜•)?",
    r"ê³„ì—´\s*ì í•©(?:\s*ì „í˜•)?",
    r"êµ­ì œ\s*í˜•(?:\s*ì „í˜•)?",
    r"ì¶”ì²œ\s*í˜•(?:\s*ì „í˜•)?",
]

KEY_ELEMENT_WORDS = [
    "ì„œë¥˜", "ë©´ì ‘", "ë…¼ìˆ ", "ìˆ˜ëŠ¥", "ìµœì €", "í•™ìƒë¶€", "êµê³¼", "ë¹„êµê³¼", "í™œë™", "ìê¸°ì†Œê°œì„œ",
    "ì¶”ì²œ", "ì‹¤ê¸°", "ì‹¤ì ", "ì¶œê²°", "ë´‰ì‚¬", "ì„¸íŠ¹", "ì „ê³µì í•©", "í•™ì—…ì—­ëŸ‰", "ë°œì „ê°€ëŠ¥ì„±"
]

def find_percent_ratios(block: str) -> List[str]:
    # ì˜ˆ: "êµê³¼ 90% + ì„œë¥˜ 10%", "ì„œë¥˜ 100%", "1ë‹¨ê³„: ì„œë¥˜ 100%, 2ë‹¨ê³„: 1ë‹¨ê³„ 70% + ë©´ì ‘ 30%"
    ratios = re.findall(r"(?:ì„œë¥˜|ë©´ì ‘|ë…¼ìˆ |êµê³¼|í•™ìƒë¶€|ì‹¤ê¸°|ìˆ˜ëŠ¥)\s*\d{1,3}\s*%|(?:\d{1,3}\s*%)", block)
    # ì¤‘ë³µ/ë…¸ì´ì¦ˆ ì¤„ì´ê¸°
    cleaned = []
    for r0 in ratios:
        r0 = r0.strip()
        if r0 not in cleaned:
            cleaned.append(r0)
    return cleaned[:12]

def find_schedule_lines(text: str) -> List[str]:
    # ì¼ì • ê´€ë ¨ íŒ¨í„´: ì›ì„œì ‘ìˆ˜/ì„œë¥˜ì œì¶œ/1ë‹¨ê³„ë°œí‘œ/ë©´ì ‘/ë…¼ìˆ /ìµœì¢…ë°œí‘œ ë“±
    lines = [ln.strip() for ln in text.split("\n") if ln.strip()]
    hits = []
    for ln in lines:
        if any(k in ln for k in ["ì›ì„œ", "ì ‘ìˆ˜", "ì„œë¥˜", "ì œì¶œ", "ë°œí‘œ", "ë©´ì ‘", "ë…¼ìˆ ", "ì‹¤ê¸°", "ë“±ë¡"]):
            if re.search(r"\d{1,2}\s*ì›”|\d{1,2}\.\d{1,2}|\d{4}\.\d{1,2}", ln):
                hits.append(ln)
    # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒìœ„ ëª‡ ê°œë§Œ
    return hits[:20]

def detect_csat_min(block: str) -> str:
    # ìˆ˜ëŠ¥ìµœì €/ìµœì €í•™ë ¥ê¸°ì¤€/ì‘ì‹œì˜ì—­ ë“±
    if re.search(r"ìˆ˜ëŠ¥\s*ìµœì €|ìµœì €\s*í•™ë ¥|ìµœì €í•™ë ¥ê¸°ì¤€", block):
        # êµ¬ì²´ ë¬¸ì¥ ì¼ë¶€ë§Œ ì¶”ì¶œ
        m = re.search(r"(ìˆ˜ëŠ¥\s*ìµœì €[^\n\.]{0,120}|ìµœì €\s*í•™ë ¥[^\n\.]{0,120}|ìµœì €í•™ë ¥ê¸°ì¤€[^\n\.]{0,120})", block)
        return (m.group(0).strip() if m else "ìˆ˜ëŠ¥ìµœì € ìˆìŒ(ìƒì„¸ëŠ” ìš”ê°• í™•ì¸)")
    if re.search(r"ìˆ˜ëŠ¥\s*ìµœì €\s*ì—†|ìµœì €\s*ì—†", block):
        return "ìˆ˜ëŠ¥ìµœì € ì—†ìŒ"
    return "ìš”ê°• í™•ì¸"

def extract_key_elements(block: str) -> List[str]:
    found = []
    for w in KEY_ELEMENT_WORDS:
        if w in block and w not in found:
            found.append(w)
    # â€œìš”ì†Œâ€ëŠ” ë„ˆë¬´ ë§ì•„ì§€ë©´ ì˜ë¯¸ ì—†ìœ¼ë‹ˆ ìƒìœ„ë§Œ
    return found[:12]

def split_into_sections(text: str) -> List[str]:
    # ì „í˜• ë‹¨ìœ„ë¡œ ìë¥´ê¸°: ì „í˜•ëª… í›„ë³´ê°€ ë“±ì¥í•˜ëŠ” ì§€ì  ê¸°ì¤€
    # 1) ì¤„ ë‹¨ìœ„ë¡œ ìŠ¤ìº”í•˜ë©° ì „í˜•ëª… ë§¤ì¹˜ë˜ëŠ” ì¤„ì„ í—¤ë”ë¡œ ê°„ì£¼
    lines = text.split("\n")
    header_idxs = []
    header_regex = re.compile("|".join(TRACK_NAME_PATTERNS))
    for i, ln in enumerate(lines):
        if header_regex.search(ln.replace(" ", "")) or header_regex.search(ln):
            header_idxs.append(i)

    if not header_idxs:
        # ëª» ì°¾ìœ¼ë©´ ì „ì²´ë¥¼ í•œ ë©ì–´ë¦¬ë¡œ
        return [text]

    header_idxs = sorted(set(header_idxs))
    sections = []
    for j, idx in enumerate(header_idxs):
        start = idx
        end = header_idxs[j+1] if j+1 < len(header_idxs) else len(lines)
        sec = "\n".join(lines[start:end]).strip()
        if len(sec) > 60:
            sections.append(sec)
    return sections[:60]  # ê³¼ë„ ë°©ì§€

def regex_parse_routes(university: str, year: str, text: str, source_file: str) -> Dict[str, Any]:
    text = normalize_space(text)
    sections = split_into_sections(text)

    # ì¼ì •ì€ ì „ì²´ì—ì„œ ë½‘ì•„ â€œê³µí†µ ì¼ì •â€ìœ¼ë¡œë„ ì €ì¥(í•„ìš” ì‹œ ì „í˜•ë³„ì— ë¶™ì„)
    common_schedule = find_schedule_lines(text)

    routes = []
    header_regex = re.compile("|".join(TRACK_NAME_PATTERNS))

    for sec in sections:
        # ì „í˜•ëª… í›„ë³´: ì„¹ì…˜ ì²« ì¤„/ì´ˆë°˜ì—ì„œ ì¡ê¸°
        head = sec.split("\n")[0][:80]
        m = header_regex.search(head) or header_regex.search(sec[:200])
        if not m:
            continue
        route_name = re.sub(r"\s+", " ", m.group(0)).strip()

        # ì „í˜•ìš”ì†Œ/ë°˜ì˜ë¹„ìœ¨/ìˆ˜ëŠ¥ìµœì €
        elements = extract_key_elements(sec)
        ratios = find_percent_ratios(sec)
        csat_min = detect_csat_min(sec)

        # ì„¹ì…˜ ë‚´ â€œì „í˜•ìš”ì†Œâ€ ë¬¸ì¥ ì¼ë¶€ ìš”ì•½(ì •ê·œì‹)
        # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ notesë¡œ
        notes = sec[:900]
        notes = re.sub(r"\s+", " ", notes).strip()

        routes.append({
            "university": university,
            "year": year,
            "route_name": route_name,
            "key_elements": elements,
            "evaluation_ratio": ratios,   # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(ì›ë¬¸ì—ì„œ ì¡íŒ % ì¡°ê°ë“¤)
            "csat_minimum": csat_min,
            "schedule": common_schedule[:8],  # ê³µí†µ ì¼ì • ìƒìœ„ë§Œ
            "source": {"file": source_file},
            "confidence": "regex",
            "notes": notes
        })

    # ì¤‘ë³µ ì „í˜•ëª… í•©ì¹˜ê¸°(ê°€ì¥ ì •ë³´ ë§ì€ ê²ƒ ìš°ì„ )
    merged = {}
    for r in routes:
        k = r["route_name"]
        if k not in merged:
            merged[k] = r
        else:
            # ì •ë³´ëŸ‰ ë¹„êµ í›„ ë” í’ë¶€í•œ ìª½ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            def info_score(x):
                return len(x.get("key_elements", [])) + len(x.get("evaluation_ratio", [])) + len(" ".join(x.get("schedule", [])))
            if info_score(r) > info_score(merged[k]):
                merged[k] = r

    return {
        "university": university,
        "year": year,
        "source_file": source_file,
        "routes": list(merged.values()),
        "common_schedule": common_schedule,
        "parser": {"mode": "regex", "route_count": len(merged)}
    }

# -----------------------------
# LLM refine (Hybrid: regex output -> structured cleanup)
# -----------------------------
def llm_refine_routes(client, model: str, parsed: Dict[str, Any], raw_text: str) -> Dict[str, Any]:
    if client is None:
        return parsed

    system = (
        "ë„ˆëŠ” í•œêµ­ ëŒ€í•™ ì…í•™ì „í˜•(ìˆ˜ì‹œ) ëª¨ì§‘ìš”ê°•ì„ êµ¬ì¡°í™”í•˜ëŠ” ë°ì´í„° ì—”ì§€ë‹ˆì–´ë‹¤. "
        "ì£¼ì–´ì§„ (1) ì •ê·œì‹ 1ì°¨ ì¶”ì¶œ ê²°ê³¼ì™€ (2) ì›ë¬¸ ë°œì·Œë¥¼ ì°¸ê³ í•˜ì—¬, ì „í˜• ì •ë³´ë¥¼ ë” ì •í™•í•˜ê³  ì¼ê´€ëœ JSONìœ¼ë¡œ ì •ë¦¬í•œë‹¤. "
        "ì •ë³´ê°€ ë¶ˆëª…í™•í•˜ë©´ 'ìš”ê°• í™•ì¸'ì´ë¼ê³  ì¨ë¼. ì ˆëŒ€ í™˜ê°(ì—†ëŠ” ë‚´ìš© ìƒì„±) ê¸ˆì§€."
    )

    # ì›ë¬¸ì€ ê¸¸ ìˆ˜ ìˆìœ¼ë‹ˆ ì•ë¶€ë¶„/í‚¤ì›Œë“œ ì£¼ë³€ë§Œ ì œí•œ ë°œì·Œ
    excerpt = raw_text[:8000]

    user = f"""
[ì •ê·œì‹ 1ì°¨ ì¶”ì¶œ ê²°ê³¼ JSON]
{json.dumps(parsed, ensure_ascii=False, indent=2)[:7000]}

[ëª¨ì§‘ìš”ê°• ì›ë¬¸ ë°œì·Œ]
{excerpt}

ìš”ì²­:
- routes ë°°ì—´ì„ ìœ ì§€í•˜ë˜, ê° routeì— ëŒ€í•´ ì•„ë˜ í•„ë“œë¥¼ ì •ë¦¬í•´ë¼.
  - route_name: ê°€ëŠ¥í•œ ì •í™•í•œ ì „í˜• ê³µì‹ëª…
  - key_elements: ["ì„œë¥˜","ë©´ì ‘","ë…¼ìˆ ","êµê³¼"...] ë“± í•µì‹¬ ìš”ì†Œë§Œ
  - evaluation_ratio: ê°€ëŠ¥í•˜ë©´ "ì„œë¥˜ 100%", "êµê³¼ 90% + ì„œë¥˜ 10%" ê°™ì€ ë¬¸ì¥ í˜•íƒœë¡œ 1~3ê°œë¡œ ì •ë¦¬
  - csat_minimum: "ìˆ˜ëŠ¥ìµœì € ì—†ìŒ" ë˜ëŠ” "ìˆ˜ëŠ¥ìµœì € ìˆìŒ: (ìš”ê°• ë¬¸ì¥ ì¼ë¶€)" ë˜ëŠ” "ìš”ê°• í™•ì¸"
  - schedule: ì›ì„œ/ì„œë¥˜/1ë‹¨ê³„/ë©´ì ‘/ë…¼ìˆ /ìµœì¢… ë°œí‘œ ë“± í•µì‹¬ ì¼ì • 3~8ê°œ
  - notes: ì¤‘ë³µì§€ì› ì œí•œ/ì¶”ì²œ í•„ìš”/ìê²© ìš”ê±´ ë“± ì£¼ì˜ì‚¬í•­ ìš”ì•½ 1~3ì¤„
- JSONë§Œ ì¶œë ¥(ì½”ë“œíœìŠ¤ ì—†ì´).
"""

    refined = llm_json(client, model, system, user)
    if refined and isinstance(refined, dict) and "routes" in refined:
        # í‘œì¤€ í•„ë“œê°€ ë¹ ì¡Œì„ ë•Œ ë³´ì™„
        refined.setdefault("university", parsed.get("university"))
        refined.setdefault("year", parsed.get("year"))
        refined.setdefault("source_file", parsed.get("source_file"))
        refined.setdefault("common_schedule", parsed.get("common_schedule", []))
        refined.setdefault("parser", {"mode": "hybrid", "route_count": len(refined.get("routes", []))})
        # confidence ë§ˆí¬
        for r in refined.get("routes", []):
            r["confidence"] = "hybrid"
        return refined

    return parsed

# -----------------------------
# KB utilities: routes -> DataFrame
# -----------------------------
def routes_to_df(routes: List[Dict[str, Any]]) -> pd.DataFrame:
    rows = []
    for r in routes:
        rows.append({
            "ì „í˜•ëª…(route_name)": r.get("route_name", ""),
            "ì „í˜•ìš”ì†Œ(key_elements)": ", ".join(r.get("key_elements", []) or []),
            "ë°˜ì˜ë¹„ìœ¨(evaluation_ratio)": " | ".join(r.get("evaluation_ratio", []) or []),
            "ìˆ˜ëŠ¥ìµœì €(csat_minimum)": r.get("csat_minimum", ""),
            "ì¼ì •(schedule)": " / ".join(r.get("schedule", []) or []),
            "ì£¼ì˜ì‚¬í•­(notes)": r.get("notes", ""),
            "confidence": r.get("confidence", "")
        })
    return pd.DataFrame(rows)

def upsert_university_kb(kb: Dict[str, Any], uni: str, year: str, payload: Dict[str, Any]) -> None:
    kb.setdefault("universities", {})
    kb["universities"].setdefault(uni, {})
    kb["universities"][uni][year] = payload

def get_university_routes(kb: Dict[str, Any], uni: str, year: str) -> List[Dict[str, Any]]:
    try:
        return kb["universities"][uni][year]["routes"]
    except Exception:
        return []

# -----------------------------
# Recommendation logic (KB ê¸°ë°˜)
#   - KBê°€ ì—†ìœ¼ë©´ fallback(ê°„ì´ ë£°)ë¡œë¼ë„ ì¶”ì²œ
# -----------------------------
def score_route_by_profile(route: Dict[str, Any], profile: Dict[str, Any]) -> float:
    gpa = profile["gpa"]              # 1~9
    ecs = profile["ecs_strength"]     # 1~5
    interview = profile["interview"]  # 1~5
    nonsul = profile["nonsul"]        # 1~5
    intl = profile["international"]   # 1~5
    reco_ok = profile["reco_ok"]      # bool
    qualification = profile["qualification"]  # bool

    name = route.get("route_name", "")
    elems = route.get("key_elements", []) or []
    csat = route.get("csat_minimum", "")
    notes = route.get("notes", "") or ""
    ratio_text = " ".join(route.get("evaluation_ratio", []) or [])

    s = 0.0

    # ì „í˜• íƒ€ì… ê°ì§€(ì´ë¦„/ìš”ì†Œ ê¸°ë°˜)
    is_gyogwa = ("êµê³¼" in name) or ("í•™êµì¶”ì²œ" in name) or ("ì¶”ì²œ" in name and "ì¢…í•©" not in name)
    is_jonghap = ("ì¢…í•©" in name) or ("ê³„ì—´" in name) or ("í™œë™" in name) or ("ì¼ë°˜" in name)
    is_nonsul = ("ë…¼ìˆ " in name) or ("ë…¼ìˆ " in elems)

    is_reco = ("ì¶”ì²œ" in name) or ("í•™êµì¥" in notes) or ("ì¶”ì²œ" in notes)
    is_gw = ("ê¸°íšŒ" in name) or ("ê³ ë¥¸ê¸°íšŒ" in name) or ("ë‹¤ë¬¸í™”" in name) or ("ì •ì›ì™¸" in notes)

    # ì¶”ì²œ í•„ìš” ì „í˜•ì¸ë° ì¶”ì²œ ë¶ˆê°€ë©´ í˜ë„í‹°
    if is_reco and not reco_ok:
        s -= 2.0
    if is_reco and reco_ok:
        s += 0.6

    # ê¸°íšŒ/ìê²© ì „í˜• ê°€ì‚°
    if is_gw and qualification:
        s += 2.0
    if is_gw and not qualification:
        s -= 0.6

    # ë‚´ì‹  ë¯¼ê°ë„(êµê³¼/êµê³¼ë¹„ì¤‘ì´ í° ê²ƒ)
    # ratio_textì— êµê³¼ 70~100% ì–¸ê¸‰ ìˆìœ¼ë©´ ë‚´ì‹  ë¯¼ê°
    gpa_sensitive = is_gyogwa or bool(re.search(r"êµê³¼\s*\d{1,3}\s*%", ratio_text))

    if gpa_sensitive:
        if gpa <= 2.5: s += 2.2
        elif gpa <= 4.0: s += 1.1
        elif gpa <= 6.0: s += 0.0
        else: s -= 1.2
    else:
        # ì¢…í•©/ë…¼ìˆ ì€ ë‚´ì‹  ë‚®ì•„ë„ ì—¬ì§€
        if gpa >= 5.0 and (is_jonghap or is_nonsul):
            s += 0.6

    # ë¹„êµê³¼/ë©´ì ‘/ë…¼ìˆ  ì—­ëŸ‰
    if is_jonghap:
        s += (ecs - 3) * 0.9
    if "ë©´ì ‘" in elems or "ë©´ì ‘" in ratio_text or "ë©´ì ‘" in notes:
        s += (interview - 3) * 0.7
    if is_nonsul:
        s += (nonsul - 3) * 1.0

    # êµ­ì œí˜•/ê¸€ë¡œë²Œ
    if "êµ­ì œ" in name:
        s += (intl - 3) * 0.9

    # ìˆ˜ëŠ¥ìµœì € ë¦¬ìŠ¤í¬(ì‚¬ìš©ì ì…ë ¥ì´ ì—†ìœ¼ë‹ˆ, ê¸°ë³¸ì ìœ¼ë¡œ â€œìµœì € ìˆìŒâ€ì€ ì•½ê°„ í˜ë„í‹°)
    if "ìˆ˜ëŠ¥ìµœì € ìˆìŒ" in csat or "ìµœì €" in csat and "ì—†ìŒ" not in csat:
        s -= 0.3

    return s

# -----------------------------
# 8-week roadmap (í…œí”Œë¦¿ + LLM optional)
# -----------------------------
def roadmap_template(uni: str, route_name: str) -> List[Dict[str, Any]]:
    is_nonsul = "ë…¼ìˆ " in route_name
    is_gyogwa = ("êµê³¼" in route_name) or ("í•™êµì¶”ì²œ" in route_name) or ("ì¶”ì²œ" in route_name and "ì¢…í•©" not in route_name)
    is_jonghap = ("ì¢…í•©" in route_name) or ("ê³„ì—´" in route_name) or ("í™œë™" in route_name) or ("ì¼ë°˜" in route_name)
    is_gw = ("ê¸°íšŒ" in route_name) or ("ê³ ë¥¸ê¸°íšŒ" in route_name)

    plan = []
    for w in range(1, 9):
        p = {"week": w, "goal": "", "tasks": []}
        if w == 1:
            p["goal"] = "ìš”ê°• ì²´í¬ë¦¬ìŠ¤íŠ¸ + ë‚´ ìŠ¤í™ ê°­ ë¶„ì„"
            p["tasks"] = [
                f"{uni} {route_name} ìš”ê°•ì—ì„œ ìê²©/ì œì¶œ/ì¼ì •/ìµœì € ì²´í¬ë¦¬ìŠ¤íŠ¸(1p) ë§Œë“¤ê¸°",
                "ë‚´ì‹ /ë¹„êµê³¼/ìˆ˜ëŠ¥/ë©´ì ‘/ë…¼ìˆ  í˜„í™©ì„ â€˜ê°€ëŠ¥-ë¦¬ìŠ¤í¬â€™ë¡œ í‘œê¸°(ê°­í‘œ)",
                "ì§€ì›í•™ê³¼ 3~5ê°œ í›„ë³´ í™•ì • + ì „í˜•ë³„ ë¦¬ìŠ¤í¬(ì¶”ì²œ/ìµœì €/ë©´ì ‘) í‘œì‹œ"
            ]
        elif w == 2:
            p["goal"] = "ì§€ì›ë™ê¸°/ì „ê³µì í•© ìŠ¤í† ë¦¬ ì´ˆì•ˆ"
            p["tasks"] = [
                "í™œë™ 3ê°œë¥¼ â€˜ë¬¸ì œ-í–‰ë™-ì„±ê³¼-ë°°ì›€â€™ êµ¬ì¡°ë¡œ ì¬ì„œìˆ (ê·¼ê±° ì¤‘ì‹¬)",
                "ì „ê³µ ê´€ë ¨ íƒêµ¬/ë…ì„œ/í”„ë¡œì íŠ¸ ê·¼ê±° 3ê°œ í™•ì •(ë§í¬/ìë£Œ í¬í•¨)",
                "ë©´ì ‘ ëŒ€ë¹„ìš© 1ë¶„ ìê¸°ì†Œê°œ + ê¼¬ë¦¬ì§ˆë¬¸ 10ê°œ ì„¸íŠ¸"
            ]
        elif w == 3:
            if is_nonsul:
                p["goal"] = "ë…¼ìˆ  ìœ í˜• ë¶„ì„ + ê¸°ì¶œ 2~3íšŒ"
                p["tasks"] = [
                    "í•´ë‹¹ ëŒ€í•™ ë…¼ìˆ  ê¸°ì¶œ ìœ í˜•/ì±„ì  í¬ì¸íŠ¸ 1p ìš”ì•½",
                    "ê¸°ì¶œ 2íšŒ â€˜ì‹¤ì „ ì‹œê°„â€™ìœ¼ë¡œ ì‘ì„± + ìê¸° ì²¨ì‚­ ì²´í¬ë¦¬ìŠ¤íŠ¸",
                    "ìì£¼ ë¬´ë„ˆì§€ëŠ” í¬ì¸íŠ¸(ê°œìš”/ê·¼ê±°/ë¬¸ì¥/ì‹œê°„) 5ê°œ ê·œì¹™í™”"
                ]
            else:
                p["goal"] = "ì„œë¥˜/ë©´ì ‘ ì‹¤ì „í™”(ì¦ê±° ì¤‘ì‹¬)"
                p["tasks"] = [
                    "ì„œë¥˜ ë¬¸ì¥ â€˜ì¶”ìƒâ†’êµ¬ì²´â€™ë¡œ ë¦¬ë¼ì´íŒ…(ìˆ«ì/ì—­í• /ê²°ê³¼ í¬í•¨)",
                    "ì˜ˆìƒì§ˆë¬¸ 20ê°œ ë‹µë³€ì„ â€˜ê²°ë¡ -ê·¼ê±°-ì˜ˆì‹œâ€™ í¬ë§·ìœ¼ë¡œ í†µì¼",
                    "ëª¨ì˜ë©´ì ‘ 1íšŒ(ë…¹í™”) í›„ í”¼ë“œë°± 10ê°œ ë°˜ì˜"
                ]
        elif w == 4:
            if is_gyogwa:
                p["goal"] = "êµê³¼ ê°•ì  ê·¹ëŒ€í™” + ìµœì €/ì¶”ì²œ ë¦¬ìŠ¤í¬ ê´€ë¦¬"
                p["tasks"] = [
                    "ë°˜ì˜ ê³¼ëª©/ê°€ì¤‘ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê°•ì  ê³¼ëª©í‘œ ë§Œë“¤ê¸°",
                    "ìˆ˜ëŠ¥ìµœì €ê°€ ìˆë‹¤ë©´ 4ì£¼ ë‹¨ê¸° ìµœì € í”Œëœ(ì˜ì—­ë³„ ëª©í‘œ) ì„¤ì •",
                    "ì¶”ì²œ í•„ìš” ì‹œ ë‹´ì„/ì§„í•™ë¶€ ì»¨íƒ ì¼ì • í™•ì •(ì¶”ì²œ ê°€ëŠ¥ì„± ì²´í¬)"
                ]
            else:
                p["goal"] = "ë§ì¶¤í˜• ì§€ì› ì „ëµ ê³ ë„í™”"
                p["tasks"] = [
                    "í•™ê³¼ë³„ â€˜ì™œ ì—¬ê¸°?â€™ ë¬¸ì¥ 3ê°œì”©(ê·¼ê±° í¬í•¨) ì œì‘",
                    "í¬íŠ¸í´ë¦¬ì˜¤/ì¦ë¹™ ìë£Œ í´ë” ì •ë¦¬(íŒŒì¼ëª… ê·œì¹™ í†µì¼)",
                    "ëª¨ì˜ë©´ì ‘ 1íšŒ(ë˜ëŠ” ë…¼ìˆ  1íšŒ) ì¶”ê°€ + ì•½ì  ë³´ì™„ ê³„íš"
                ]
        elif w == 5:
            p["goal"] = "ìˆ˜ì‹œ 6ì¥ ì¡°í•© í™•ì •(í”ŒëœA/B)"
            p["tasks"] = [
                "ìƒí–¥/ì ì •/ì•ˆì • ë°¸ëŸ°ìŠ¤í‘œ ì‘ì„±(ê° ì „í˜•ì˜ í•©ê²© í¬ì¸íŠ¸ 3ì¤„)",
                "ê°€ì¥ ì•½í•œ êµ¬ê°„ 1ê°œë¥¼ ì„ ì •í•´ 2ì£¼ ì§‘ì¤‘ ë³´ì™„(ë©´ì ‘/ë…¼ìˆ /ìµœì €)",
                "ì œì¶œë¬¼/ì„œë¥˜ ì‘ì—… ì¼ì •í‘œ(ë§ˆê° ì—­ì‚°) ì™„ì„±"
            ]
        elif w == 6:
            if is_nonsul:
                p["goal"] = "ë…¼ìˆ  ì‹¤ì „ ì£¼ê°„(ëª¨ì˜ 3íšŒ)"
                p["tasks"] = [
                    "ê¸°ì¶œ 2íšŒ + ëª¨ì˜ 1íšŒ(ì‹¤ì „ ì‹œê°„/í™˜ê²½) ìˆ˜í–‰",
                    "ì²¨ì‚­ ì²´í¬: ë…¼ì§€ ì¼ê´€/ê·¼ê±° ì§ˆ/ë¬¸ì¥ ëª…ë£Œ/ì‹œê°„ ë°°ë¶„",
                    "ê¸ˆì§€ íŒ¨í„´(5ê°œ) í™•ì • + ìµœì¢… í…œí”Œë¦¿(ê°œìš” êµ¬ì¡°) ì™„ì„±"
                ]
            else:
                p["goal"] = "ë©´ì ‘/ì„œë¥˜ ìµœì¢… ì••ì¶•"
                p["tasks"] = [
                    "ë¹ˆì¶œ ì§ˆë¬¸(ë™ê¸°/ì „ê³µ/í˜‘ì—…/ê°ˆë“±/ì„±ì¥) ìŠ¤í¬ë¦½íŠ¸ ìµœì¢…ë³¸",
                    "ë‚´ í™œë™ì˜ â€˜ê°ê´€ì  í‘œí˜„â€™ 10ë¬¸ì¥(ì—­í• /ì„±ê³¼/ì§€í‘œ) ì •ë¦¬",
                    "ëª¨ì˜ë©´ì ‘ 2íšŒ(ë˜ëŠ” ì§ˆì˜ì‘ë‹µ 30ë¬¸í•­)ë¡œ ì•ˆì •í™”"
                ]
        elif w == 7:
            p["goal"] = "ì œì¶œ/ì›ì„œ ì‹¤ìˆ˜ ë°©ì§€ ì²´í¬ ì™„ë£Œ"
            p["tasks"] = [
                "íŒŒì¼/ì–‘ì‹/ê¸€ììˆ˜/ê°œì¸ì •ë³´/ì¦ë¹™ ëˆ„ë½ ìµœì¢… ì ê²€í‘œ ì™„ë£Œ",
                "ì¶”ì²œ/í•™êµ ì œì¶œ(í•´ë‹¹ ì‹œ) ë§ˆê° ì „ ì™„ë£Œ(ì¦ë¹™ ìº¡ì²˜/í™•ì¸)",
                "ìµœì¢…ë³¸ ë°±ì—…(í´ë¼ìš°ë“œ+ë¡œì»¬) + íŒŒì¼ëª… ê·œì¹™ í™•ì •"
            ]
        elif w == 8:
            p["goal"] = "ì‹¤ì „ ì»¨ë””ì…˜ ì„¸íŒ… + í”ŒëœB(ì •ì‹œ/ì¶”ê°€í•©ê²©) ì¤€ë¹„"
            p["tasks"] = [
                "ì‹œí—˜/ë©´ì ‘ ë‹¹ì¼ ë£¨í‹´(ìˆ˜ë©´/ì‹ì‚¬/ì´ë™/ì¤€ë¹„ë¬¼) ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¶œë ¥",
                "ëŒ€í•™ë³„ 10ë¶„ ìš”ì•½ ë…¸íŠ¸(ì „ê³µ/í™œë™/ì§ˆë¬¸) ì œì‘",
                "ê²°ê³¼ ëŒ€ê¸° í”Œëœ: ì¶”ê°€í•©ê²© ëŒ€ì‘ + ì •ì‹œ ì „í™˜ ì²´í¬ë¦¬ìŠ¤íŠ¸"
            ]

        if is_gw:
            p["tasks"].append("â€» (ìê²© ì „í˜•) ìê²©/ì¦ë¹™ ëˆ„ë½ì´ ì¦‰íƒˆ í¬ì¸íŠ¸ â†’ ì¦ë¹™ ì²´í¬ë¥¼ í•­ìƒ 1ìˆœìœ„ë¡œ ê³ ì •")
        if is_jonghap:
            p["tasks"].append("â€» (í•™ì¢…) â€˜í™œë™ ë‚˜ì—´â€™ ê¸ˆì§€: ëª¨ë“  ë¬¸ì¥ì„ ê·¼ê±°/ì—­í• /ê²°ê³¼ë¡œ ì¦ëª…")

        plan.append(p)
    return plan

def roadmap_llm_refine(client, model: str, uni: str, route: Dict[str, Any], profile: Dict[str, Any]) -> List[Dict[str, Any]]:
    base = roadmap_template(uni, route.get("route_name", ""))
    if client is None:
        return base

    system = "ë„ˆëŠ” í•œêµ­ ì…ì‹œ ì½”ì¹˜ë‹¤. ì „í˜• ì¡°ê±´ê³¼ ì‚¬ìš©ì í”„ë¡œí•„ì„ ë°˜ì˜í•´ 8ì£¼ ë¡œë“œë§µì„ ë§¤ìš° êµ¬ì²´ì  ì‚°ì¶œë¬¼ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•œë‹¤."
    user = f"""
ëŒ€í•™: {uni}
ì „í˜•: {route.get('route_name')}
ì „í˜• ì •ë³´:
- ì „í˜•ìš”ì†Œ: {route.get('key_elements')}
- ë°˜ì˜ë¹„ìœ¨: {route.get('evaluation_ratio')}
- ìˆ˜ëŠ¥ìµœì €: {route.get('csat_minimum')}
- ì¼ì •: {route.get('schedule')}
- ì£¼ì˜ì‚¬í•­: {route.get('notes')}

ì‚¬ìš©ì í”„ë¡œí•„:
{json.dumps(profile, ensure_ascii=False, indent=2)}

ìš”ì²­:
- 8ì£¼ ë¡œë“œë§µì„ JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥í•˜ë¼.
- ê° ì›ì†Œ: {{"week":1,"goal":"...","tasks":["..."]}}
- tasksëŠ” â€œì‚°ì¶œë¬¼ ì¤‘ì‹¬â€ (ì²´í¬ë¦¬ìŠ¤íŠ¸/ìŠ¤í¬ë¦½íŠ¸/ê¸°ì¶œ níšŒ/í´ë”ì •ë¦¬ ë“±)
"""
    refined = llm_json(client, model, system, user)
    if isinstance(refined, list) and len(refined) == 8:
        return refined
    return base

# -----------------------------
# Sidebar
# -----------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    api_key = st.text_input("OpenAI API Key (ì„ íƒ)", type="password", help="ë„£ìœ¼ë©´: PDF íŒŒì‹± ì •êµí™”(2ì°¨) + 8ì£¼ í”Œëœ ë¬¸ì¥ë ¥ì´ í™• ì˜¬ë¼ê°€ìš”.")
    model = st.selectbox("ëª¨ë¸", ["gpt-4o-mini", "gpt-4.1-mini", "gpt-4o"], index=0)
    st.divider()
    st.subheader("ğŸ“Œ ëŒ€í•™/ì—°ë„")
    uni = st.selectbox("ëŒ€í•™", ["ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€"], index=1)
    year = st.selectbox("ì—°ë„", ["2026"], index=0)

client = get_openai_client(api_key) if api_key else None

# -----------------------------
# Tabs
# -----------------------------
tab1, tab2, tab3 = st.tabs(["ğŸ“š ì „í˜• ì •ë³´(KB) êµ¬ì¶•", "ğŸ§­ ì „í˜• ì¶”ì²œ + 8ì£¼ ë¡œë“œë§µ", "ğŸ—‚ï¸ DB ê´€ë¦¬/ë‚´ë³´ë‚´ê¸°"])

# =========================================================
# TAB 1: KB êµ¬ì¶• (PDF -> regex parse -> optional LLM refine -> save JSON)
# =========================================================
with tab1:
    st.subheader("ğŸ“š ì „í˜• ì •ë³´(KB) â€” ëª¨ì§‘ìš”ê°• PDF ì—…ë¡œë“œ â†’ ì „í˜•í‘œ ìë™ ìƒì„±(ì •ê·œì‹+LLM)")
    st.write(
        "ë„ˆê°€ ë§í•œ í•µì‹¬(â€œëª°ë¼ì„œ ì§€ì› ëª»í•˜ëŠ” ì‚¬ëŒâ€)ì„ í•´ê²°í•˜ëŠ” ë ˆì´ì–´ì•¼. "
        "PDFì—ì„œ ì „í˜• ì •ë³´ë¥¼ êµ¬ì¡°í™”í•´ì„œ ì €ì¥í•´ë‘ë©´, ì´í›„ ì¶”ì²œ/ì„¤ëª…ì´ **ê·¼ê±° ê¸°ë°˜**ìœ¼ë¡œ ëŒì•„ê°€."
    )

    kb = load_kb()
    uploaded_files = st.file_uploader(
        "ì„œìš¸ëŒ€/ì—°ì„¸ëŒ€/ê³ ë ¤ëŒ€ 2026 ìˆ˜ì‹œ ëª¨ì§‘ìš”ê°• PDF ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
        type=["pdf"],
        accept_multiple_files=True
    )

    colA, colB = st.columns([1, 1])
    with colA:
        use_llm_refine = st.checkbox("LLMìœ¼ë¡œ íŒŒì‹± ê²°ê³¼ ì •êµí™”(ê¶Œì¥)", value=bool(api_key))
        st.caption("ì •ê·œì‹ 1ì°¨ â†’ LLM 2ì°¨ ë³´ì •(í™˜ê° ê¸ˆì§€ í”„ë¡¬í”„íŠ¸)ë¡œ â€˜ë°˜ì˜ë¹„ìœ¨/ìµœì €/ì£¼ì˜ì‚¬í•­â€™ì´ ë” ê¹”ë”í•´ì ¸ìš”.")

    with colB:
        st.info("âš ï¸ ìŠ¤ìº”ë³¸(ì´ë¯¸ì§€) PDFëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì˜ ì•ˆ ë  ìˆ˜ ìˆì–´ìš”. (ì´ˆê¸° MVPëŠ” í…ìŠ¤íŠ¸ PDF ìš°ì„  ì§€ì›)")

    if st.button("ğŸ“Œ ì—…ë¡œë“œ PDF ë¶„ì„ â†’ KB ì €ì¥", type="primary", use_container_width=True):
        if not uploaded_files:
            st.error("PDFë¥¼ ì—…ë¡œë“œí•´ì¤˜.")
        else:
            all_routes_preview = []
            with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ + ì „í˜• íŒŒì‹± ì¤‘..."):
                for f in uploaded_files:
                    raw = extract_text_from_pdf(f)
                    if not raw or len(raw) < 200:
                        st.warning(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨/ë¶€ì¡±: {f.name} (ìŠ¤ìº”ë³¸ì¼ ê°€ëŠ¥ì„±)")
                        continue

                    parsed = regex_parse_routes(uni, year, raw, source_file=f.name)

                    if use_llm_refine and client is not None:
                        parsed = llm_refine_routes(client, model, parsed, raw)

                    # KB upsert
                    upsert_university_kb(kb, uni, year, parsed)

                    # preview
                    for r in parsed.get("routes", []):
                        all_routes_preview.append(r)

            save_kb(kb)
            st.success(f"KB ì €ì¥ ì™„ë£Œ âœ…  ({uni} {year}) â€” ì „í˜• {len(all_routes_preview)}ê°œ íŒŒì‹±/ì €ì¥")

    # Show current KB summary
    kb = load_kb()
    routes = get_university_routes(kb, uni, year)
    st.markdown("### âœ… í˜„ì¬ ì €ì¥ëœ ì „í˜• í‘œ")
    if routes:
        df = routes_to_df(routes)
        st.dataframe(df, use_container_width=True, hide_index=True)
        st.caption("â€» confidence=hybridë©´ â€˜ì •ê·œì‹+LLM ë³´ì •â€™ì´ ì ìš©ëœ ìƒíƒœ.")
    else:
        st.warning("ì•„ì§ KBê°€ ë¹„ì–´ìˆì–´. ìœ„ì—ì„œ PDF ë¶„ì„â†’KB ì €ì¥ì„ ë¨¼ì € í•´ì¤˜.")

    # Allow manual edit (light)
    st.markdown("### âœï¸ (ì„ íƒ) ì „í˜• ë°ì´í„° ìˆ˜ë™ ë³´ì •")
    st.caption("PDF íŒŒì‹±ì´ ì• ë§¤í•œ ê²½ìš°, ì—¬ê¸°ì„œ ì „í˜•ëª…/ë¹„ìœ¨/ìµœì €/ì£¼ì˜ì‚¬í•­ì„ ì§ì ‘ ìˆ˜ì •í•´ë„ ë¼.")
    if routes:
        edit_df = routes_to_df(routes).copy()
        edited = st.data_editor(edit_df, use_container_width=True, num_rows="dynamic")
        if st.button("ğŸ’¾ ìˆ˜ë™ ìˆ˜ì • ì €ì¥", use_container_width=True):
            # edited DF -> routes back (ê°„ë‹¨ ë§¤í•‘)
            new_routes = []
            for _, row in edited.iterrows():
                new_routes.append({
                    "university": uni,
                    "year": year,
                    "route_name": str(row.get("ì „í˜•ëª…(route_name)", "")).strip(),
                    "key_elements": [x.strip() for x in str(row.get("ì „í˜•ìš”ì†Œ(key_elements)", "")).split(",") if x.strip()],
                    "evaluation_ratio": [x.strip() for x in str(row.get("ë°˜ì˜ë¹„ìœ¨(evaluation_ratio)", "")).split("|") if x.strip()],
                    "csat_minimum": str(row.get("ìˆ˜ëŠ¥ìµœì €(csat_minimum)", "")).strip(),
                    "schedule": [x.strip() for x in str(row.get("ì¼ì •(schedule)", "")).split("/") if x.strip()],
                    "notes": str(row.get("ì£¼ì˜ì‚¬í•­(notes)", "")).strip(),
                    "confidence": str(row.get("confidence", "")).strip() or "manual",
                    "source": {"file": kb.get("universities", {}).get(uni, {}).get(year, {}).get("source_file", "manual")}
                })

            # overwrite routes in KB payload
            payload = kb["universities"][uni][year]
            payload["routes"] = new_routes
            payload["parser"] = {"mode": "manual_override", "route_count": len(new_routes)}
            upsert_university_kb(kb, uni, year, payload)
            save_kb(kb)
            st.success("ìˆ˜ì • ì €ì¥ ì™„ë£Œ âœ…")

# =========================================================
# TAB 2: Recommendation + Roadmap
# =========================================================
with tab2:
    st.subheader("ğŸ§­ ì „í˜• ì¶”ì²œ + 8ì£¼ ë¡œë“œë§µ")
    kb = load_kb()
    routes = get_university_routes(kb, uni, year)

    left, right = st.columns([1, 1])

    with left:
        st.markdown("### 1) ë‚´ í”„ë¡œí•„ ì…ë ¥")
        gpa = st.slider("ë‚´ì‹  ë“±ê¸‰(ëŒ€ëµ)", 1.0, 9.0, 3.5, 0.1, help="1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„ê¶Œ")
        ecs_strength = st.slider("ë¹„êµê³¼/í™œë™ ê°•ë„", 1, 5, 3)
        interview = st.slider("ë©´ì ‘ ìì‹ ê°", 1, 5, 3)
        nonsul = st.slider("ë…¼ìˆ  ìì‹ ê°(í•´ë‹¹ ì‹œ)", 1, 5, 2)
        international = st.slider("êµ­ì œ/ì–¸ì–´/ê¸€ë¡œë²Œ ì—­ëŸ‰", 1, 5, 3)
        reco_ok = st.checkbox("í•™êµ ì¶”ì²œ(ì¶”ì²œí˜•/í•™êµì¶”ì²œ ë“±) ê°€ëŠ¥", value=False)
        qualification = st.checkbox("ê¸°íšŒê· í˜•/ê³ ë¥¸ê¸°íšŒ ë“± ìê²© ì „í˜• í•´ë‹¹", value=False)

        profile = {
            "gpa": gpa,
            "ecs_strength": ecs_strength,
            "interview": interview,
            "nonsul": nonsul,
            "international": international,
            "reco_ok": reco_ok,
            "qualification": qualification,
        }

    with right:
        st.markdown("### 2) ìˆ˜ì‹œ ì„¸ë¶€ì „í˜• ì„ íƒ")
        st.caption("â€˜ì˜ ëª¨ë¥´ê² ì–´ìš”â€™ë¥¼ ê³ ë¥´ë©´ KBì—ì„œ ìë™ ì¶”ì²œí•©ë‹ˆë‹¤.")
        options = ["ì˜ ëª¨ë¥´ê² ì–´ìš”(ì¶”ì²œë°›ê¸°)"]
        if routes:
            # KBì—ì„œ ì¶”ì¶œëœ ì „í˜•ëª…
            options += sorted(list({r.get("route_name","").strip() for r in routes if r.get("route_name","").strip()}))
        else:
            options += ["í•™ìƒë¶€ì¢…í•©", "í•™ìƒë¶€êµê³¼", "ë…¼ìˆ ì „í˜•", "ê¸°íšŒê· í˜•", "íŠ¹ê¸°ì/ì‹¤ê¸°"]

        chosen = st.selectbox("ì„¸ë¶€ì „í˜•", options, index=0)

        st.markdown("### 3) ì¶”ì²œ Top3")
        if not routes:
            st.warning("KBê°€ ì—†ì–´ì„œ ì¶”ì²œì´ ì œí•œì ì´ì•¼. ë¨¼ì € â€˜ì „í˜• ì •ë³´(KB) êµ¬ì¶•â€™ íƒ­ì—ì„œ PDF ë¶„ì„â†’KB ì €ì¥ì„ í•´ì¤˜.")
        else:
            scored = []
            for r in routes:
                s = score_route_by_profile(r, profile)
                scored.append((s, r))
            scored.sort(key=lambda x: x[0], reverse=True)
            top3 = scored[:3]

            for i, (s, r) in enumerate(top3, start=1):
                st.markdown(f"#### #{i} âœ… {r.get('route_name')}")
                st.write(f"- ì „í˜•ìš”ì†Œ: {', '.join(r.get('key_elements', []) or [])}")
                ratio_txt = " | ".join(r.get("evaluation_ratio", []) or [])
                st.write(f"- ë°˜ì˜ë¹„ìœ¨: {ratio_txt if ratio_txt else 'ìš”ê°• í™•ì¸'}")
                st.write(f"- ìˆ˜ëŠ¥ìµœì €: {r.get('csat_minimum', 'ìš”ê°• í™•ì¸')}")
                if r.get("notes"):
                    st.info(r["notes"])
                st.caption(f"ì¶”ì²œ ì ìˆ˜: {s:.2f}")

            # ìµœì¢… ì „í˜• ê²°ì •
            st.divider()
            st.markdown("### 4) ìµœì¢… ì „í˜• ì„ íƒ â†’ 8ì£¼ ë¡œë“œë§µ ìƒì„±")

            if chosen == "ì˜ ëª¨ë¥´ê² ì–´ìš”(ì¶”ì²œë°›ê¸°)":
                final_route = top3[0][1] if top3 else None
                st.success(f"ìë™ ì„ íƒ: {final_route.get('route_name')}" if final_route else "ìë™ ì„ íƒ ì‹¤íŒ¨")
            else:
                # ì‚¬ìš©ìê°€ íŠ¹ì • ì „í˜•ì„ ì„ íƒí–ˆìœ¼ë©´ ê·¸ ì „í˜• ìš°ì„ 
                final_route = next((r for r in routes if r.get("route_name") == chosen), None)
                if final_route is None and top3:
                    final_route = top3[0][1]

            if final_route is None:
                st.error("ì „í˜•ì„ ê²°ì •í•  ìˆ˜ ì—†ì–´. KB/ì„ íƒì„ í™•ì¸í•´ì¤˜.")
            else:
                if st.button("ğŸ“… 8ì£¼ ë¡œë“œë§µ ë§Œë“¤ê¸°", type="primary", use_container_width=True):
                    with st.spinner("8ì£¼ ë¡œë“œë§µ ìƒì„± ì¤‘..."):
                        plan = roadmap_llm_refine(client, model, uni, final_route, profile) if client else roadmap_template(uni, final_route.get("route_name", ""))

                    st.success(f"{uni} Â· {final_route.get('route_name')} â€” 8ì£¼ ë¡œë“œë§µ ì™„ë£Œ")
                    for w in plan:
                        with st.expander(f"Week {w['week']} â€” {w['goal']}"):
                            for t in w["tasks"]:
                                st.write(f"- {t}")

                    st.download_button(
                        "â¬‡ï¸ ë¡œë“œë§µ JSON ë‹¤ìš´ë¡œë“œ",
                        data=json.dumps(plan, ensure_ascii=False, indent=2).encode("utf-8"),
                        file_name=f"Y-Compass_{uni}_{year}_{final_route.get('route_name','route')}_8weeks.json",
                        mime="application/json",
                        use_container_width=True
                    )

# =========================================================
# TAB 3: Export / Import / Reset
# =========================================================
with tab3:
    st.subheader("ğŸ—‚ï¸ DB ê´€ë¦¬/ë‚´ë³´ë‚´ê¸°")
    kb = load_kb()
    st.markdown("### í˜„ì¬ KB ìƒíƒœ")
    uni_keys = list(kb.get("universities", {}).keys())
    st.write({"universities": uni_keys, "updated_at": kb.get("updated_at")})

    st.markdown("### ğŸ“¤ KB JSON ë‹¤ìš´ë¡œë“œ")
    st.download_button(
        "â¬‡ï¸ admission_kb.json ë‹¤ìš´ë¡œë“œ",
        data=json.dumps(kb, ensure_ascii=False, indent=2).encode("utf-8"),
        file_name="admission_kb.json",
        mime="application/json",
        use_container_width=True
    )

    st.markdown("### ğŸ“¥ KB JSON ì—…ë¡œë“œ(ë³µêµ¬/ì´ì „)")
    uploaded_kb = st.file_uploader("admission_kb.json ì—…ë¡œë“œ", type=["json"])
    if uploaded_kb is not None:
        try:
            payload = json.loads(uploaded_kb.read().decode("utf-8"))
            if isinstance(payload, dict) and "universities" in payload:
                if st.button("ğŸ’¾ ì—…ë¡œë“œí•œ KBë¡œ ë®ì–´ì“°ê¸°", use_container_width=True):
                    save_kb(payload)
                    st.success("KB ë³µêµ¬/ì´ì „ ì™„ë£Œ âœ…")
            else:
                st.error("KB JSON í˜•ì‹ì´ ì•„ë‹ˆì•¼(í•„ë“œ universities í•„ìš”).")
        except Exception:
            st.error("JSON íŒŒì‹± ì‹¤íŒ¨. íŒŒì¼ì´ ê¹¨ì¡ŒëŠ”ì§€ í™•ì¸í•´ì¤˜.")

    st.markdown("### ğŸ§¨ (ì£¼ì˜) KB ì´ˆê¸°í™”")
    if st.button("KB ì´ˆê¸°í™”(ëª¨ë“  ì €ì¥ ì „í˜• ì‚­ì œ)", use_container_width=True):
        ensure_data_dir()
        if os.path.exists(KB_PATH):
            os.remove(KB_PATH)
        st.success("KB ì´ˆê¸°í™” ì™„ë£Œ. (ë‹¤ì‹œ PDF ì—…ë¡œë“œâ†’ë¶„ì„í•˜ë©´ ë¨)")

st.divider()
st.markdown("#### âœ… ì´ ë²„ì „ì—ì„œ ë„¤ ìš”ì²­ì´ ì–´ë–»ê²Œ â€˜ë°˜ì˜â€™ëëŠ”ì§€ ìš”ì•½")
st.markdown("""
- **PDF íŒŒì„œ(ì •ê·œì‹+LLM í•˜ì´ë¸Œë¦¬ë“œ)**:  
  - ì •ê·œì‹ìœ¼ë¡œ ì „í˜• ì„¹ì…˜ ê°ì§€ â†’ ì „í˜•ìš”ì†Œ/ë¹„ìœ¨/%/ìµœì €/ì¼ì • í‚¤ì›Œë“œ ì¶”ì¶œ â†’ í‘œ ìƒì„±  
  - OpenAI Key ë„£ìœ¼ë©´: LLMì´ ì¶”ì¶œ ê²°ê³¼ë¥¼ **â€˜ê³µì‹ ì „í˜•ëª…/ë¹„ìœ¨ ë¬¸ì¥/ì£¼ì˜ì‚¬í•­â€™ ì¤‘ì‹¬ìœ¼ë¡œ ì •ëˆ**(í™˜ê° ê¸ˆì§€ í”„ë¡¬í”„íŠ¸)
- **ì „í˜• DB JSON ì €ì¥/ì¦‰ì‹œ ë¡œë”©**:  
  - `data/admission_kb.json`ì— ì €ì¥ â†’ ì•± ì‹¤í–‰ ì‹œ `load_kb()`ë¡œ ì¦‰ì‹œ ë¡œë”©  
  - ë°°í¬ í™˜ê²½ì—ì„œ DBê°€ ë‚ ì•„ê°ˆ ìˆ˜ ìˆìœ¼ë‹ˆ **KB ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ(ë³µêµ¬)**ê¹Œì§€ ì œê³µ
- **â€œëª°ë¼ìš”â€ UX + ì¶”ì²œ/í”Œëœ ì—°ê²°**:  
  - â€˜ì˜ ëª¨ë¥´ê² ì–´ìš”(ì¶”ì²œë°›ê¸°)â€™ ì„ íƒ ì‹œ KBì—ì„œ ìë™ Top3 ì¶”ì²œ â†’ 8ì£¼ ë¡œë“œë§µ ìƒì„±
""")
